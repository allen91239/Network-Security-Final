# -*- coding: utf-8 -*-
"""network_security.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Id6NVtERv2Br5Rc4KRrOZL0urUIl7Eu-

### **Part 1 Log parsing**
"""

import re
import math
import numpy as np
import pandas as pd
from collections import Counter
from collections import defaultdict
import os
import sys
script_dir = os.path.dirname(__file__)
#df = pd.read_json("./Wireshark.json")
"""
def parse_test(test_path):
    file = open("./Security.xml", mode = "r", encoding='UTF-8')
    line = file.readline()
    list_of_event = list()
    while line:
      #print(line)
      line = file.readline()
      splitting = line.split("<EventID>")
      for splits in splitting:
        splits = splits.split("</EventID>")
        for split in splits:
          if len(split) == 4:
            list_of_event.append((split))
    file.close()
    file = open("./Sysmon.xml", mode = "r", encoding='UTF-8')
    line = file.readline()
    list_of_sys = list()
    while line:
      line = file.readline()
      splitting = line.split("<EventID>")
      for splits in splitting:
        splits = splits.split("</EventID>")
        for split in splits:
          if len(split) == 1 or len(split) == 2:
            list_of_sys.append((split))
    file.close()
    return list_of_event, list_of_sys
"""
def parse_log():
  #parsing log
  security_event = list()
  for i in range(6):
    path = os.path.join(script_dir,f"./Logs/Train/Person_{i+1}/Security.xml")
    #print(path)
    file = open(path, mode = "r", encoding='UTF-8')
    line = file.readline()
    list_of_event = list()
    while line:
      #print(line)
      line = file.readline()
      splitting = line.split("<EventID>")
      for splits in splitting:
        splits = splits.split("</EventID>")
        for split in splits:
          if len(split) == 4:
            list_of_event.append((split))
      #print(splitting)
    security_event.append(list(list_of_event))
    #print(list_of_event)
    file.close()
  sysmon_event = list()
  for i in range(6):
    path = os.path.join(script_dir,f"./Logs/Train/Person_{i+1}/Sysmon.xml")
    file = open(path, mode = "r", encoding='UTF-8')
    line = file.readline()
    list_of_sys = list()
    while line:
      line = file.readline()
      splitting = line.split("<EventID>")
      for splits in splitting:
        splits = splits.split("</EventID>")
        for split in splits:
          if len(split) == 1 or len(split) == 2:
            list_of_sys.append((split))
    sysmon_event.append(list(list_of_sys))
    #print(list_of_sys)
    file.close()
  #print(security_event)
  #print(sysmon_event)
  return security_event, sysmon_event

#def doc2vec(doc, vocab, dfx):
#  words = doc
#  #print([1 if token in words else 0 for token, freq in vocab])
#  return [1 if token in words else 0 for token, freq in vocab]

def doc2vec(doc, vocab, dfx):
  tfidf = np.zeros(len(vocab))
  #words = tokenize(doc)
  words = doc
  count = defaultdict(zero)
  idx = 0
  for token, freq in vocab:
    if token in words:
      for word in words:
        if word == token:
          count[token]+=1
      tfidf[idx] = count[token]/len(words) * math.log10(data_size/dfx[token])
    else:
      tfidf[idx] = 0
    idx += 1
  #print(tfidf)
  return tfidf

def zero():
  return 0
def get_vocab(corpus):
  vocabulary = Counter()
  dfx = defaultdict(zero)
  for document in corpus:
    tokens = document
    for token in set(tokens):
      dfx[token] += 1
    vocabulary.update(tokens)
  #print (dfx)
  return vocabulary, dfx

#def tokenize(document):
#  words = re.split('\W+', document)
#  return words

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)
  #print(vec_a)
  #print(vec_b)
  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0 # hack
  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i*i for i in vec_a])
  b_2 = sum([i*i for i in vec_b])
  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b, vocab, dfx):
  return cosine_similarity(doc2vec(doc_a, vocab, dfx), doc2vec(doc_b, vocab, dfx))

def k_similar(corpus, tester, vocab, dfx):
  seed_doc = tester
  #seed_doc = corpus[seed_id]
  #title_doc = title[seed_id]
  #print('> "{}"'.format(title_doc))
  similarities = [doc_similarity(seed_doc, doc, vocab, dfx) for id, doc in enumerate(corpus)]
  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-6:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
  nearest = [[id, similarities[id]] for id in top_indices]
  #print()
  #for story in reversed(nearest):
    #print('* "User{}" ({})'.format(story[0]+1, story[1]))
  return similarities

if __name__ == '__main__':
  test_path = sys.argv[1]
  data_size = 7
  security, sysmon = parse_log()
  #test1_sec, test1_sys = parse_test(test_path)
  sec_vocab, sec_dfx = get_vocab(security)
  sys_vocab, sys_dfx = get_vocab(sysmon)
  sec_vocab = sec_vocab.most_common()
  sys_vocab = sys_vocab.most_common()
  #print(sec_vocab)
  #print(test1_sec)
  #print(test1_sys)
  #print(sys_vocab)
  #print("For Security.xml:")

  tester_path = os.path.join(script_dir, "./Logs")
  tester_path = os.path.join(tester_path, test_path)
  
  sec_event = list()
  sys_event = list()
  count = 1
  for subdir, dirs, files in os.walk(tester_path):
    for filename in files:
        filepath = subdir + os.sep + filename
        #print(filepath)
        if filepath.endswith("Security.xml"):
          file = open(f"{filepath}", mode = "r", encoding='UTF-8')
          line = file.readline()
          list_of_event = list()
          while line:
            line = file.readline()
            splitting = line.split("<EventID>")
            for splits in splitting:
              splits = splits.split("</EventID>")
              for split in splits:
                if len(split) == 4:
                  list_of_event.append((split))
          
          file.close()
          sec_event = list_of_event
        elif filepath.endswith("Sysmon.xml"):
          file = open(f"{filepath}", mode = "r", encoding='UTF-8')
          line = file.readline()
          list_of_sys = list()
          while line:
            line = file.readline()
            splitting = line.split("<EventID>")
            for splits in splitting:
              splits = splits.split("</EventID>")
              for split in splits:
                if len(split) == 1 or len(split) == 2:
                  list_of_sys.append((split))
          file.close()
          sys_event = list_of_sys
        else:
            sec_sim = k_similar(security, sec_event, sec_vocab, sec_dfx)
            #print()
            #print("For Sysmon.xml:")
            sys_sim = k_similar(sysmon, sys_event, sys_vocab, sys_dfx)
            similarity = list()
            #print(sec_sim)
            #print(sys_sim)
            for sec, sys in zip(sec_sim, sys_sim):
              similarity.append(sec+sys)
            #print(similarity)
            top_indices = sorted(range(len(similarity)), key=lambda i: similarity[i])[-6:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
            nearest = [[id, similarity[id]] for id in top_indices]
            #print()
            #print("Final prediction:")
            #print()
            #for story in reversed(nearest):
              #print('* "User{}" ({})'.format(story[0]+1, story[1]))
            print(f"testcase {count}: person {nearest[5][0]+1}")
            count += 1